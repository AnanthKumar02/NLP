{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16QWv3eeZb7s",
        "outputId": "b55059aa-2dda-4b05-ca06-a1134e76d863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader"
      ],
      "metadata": {
        "id": "JO2lLDRYZkiN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PyPDF2.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yWsIcrkMZpJa",
        "outputId": "d63ea3c7-1f34-487b-d504-3c464763d874"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=open(\"/content/file1pdf.pdf\",\"rb\")\n",
        "pdf_reader=PyPDF2.PdfReader(pdf)\n",
        "\n",
        "print(\"Number of pages:\",len(pdf_reader.pages))\n",
        "page=pdf_reader.pages[1]\n",
        "print(page.extract_text())\n",
        "pdf.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwtYevYRZrI-",
        "outputId": "14df6f83-9032-43b0-9b23-2417dfbeb300"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 35\n",
            " \n",
            " \n",
            " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
            "Acknowledgements  \n",
            "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
            "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
            "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
            "2014‐34. \n",
            " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
            " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
            " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
            " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
            " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
            " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
            " \n",
            "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
            " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
            " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
            " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
            "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
            " beginning  of the project and their \n",
            "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
            "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
            "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
            "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
            "understanding  actual ground conditions.  \n",
            " \n",
            "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
            "and anticipate  the work's usefulness  for the intended purpose. \n",
            " \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function BufferedReader.close>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile = urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
        "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "FrN07feaZ4cK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2,urllib,nltk\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "icRFp3EWZtdW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTN6njRNaGHY",
        "outputId": "a67489e7-8d95-48bd-905a-e15ff682ea2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbU2t338aNAl",
        "outputId": "683f26fb-a86e-4062-fb51-e98cb3eefbab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pageObj = pdfreader.pages[2]\n",
        "page2 = pageObj.extract_text()\n",
        "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
        "tokens = word_tokenize(page2)\n",
        "stop_words = stopwords.words('english')\n",
        "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
      ],
      "metadata": {
        "id": "05LGgquCZ64k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM4JzuUQZ-zR",
        "outputId": "6ae2179e-f2ed-4a15-b4b9-25de7d00604a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐2034',\n",
              " 'Table',\n",
              " 'Contents',\n",
              " 'The',\n",
              " 'Consultant',\n",
              " 'wishes',\n",
              " 'thank',\n",
              " 'following',\n",
              " 'individuals',\n",
              " 'Municipal',\n",
              " 'Corporation',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " 'invaluable',\n",
              " 'support',\n",
              " 'insights',\n",
              " 'contributions',\n",
              " 'towards',\n",
              " '‘',\n",
              " 'Working',\n",
              " 'Paper',\n",
              " '1',\n",
              " '–',\n",
              " 'Preparation',\n",
              " 'Base',\n",
              " 'Map',\n",
              " '’',\n",
              " 'preparation',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐34',\n",
              " '.............................................................................................................................',\n",
              " '..............',\n",
              " '3',\n",
              " 'Our',\n",
              " 'gratitude',\n",
              " 'following',\n",
              " 'experts',\n",
              " 'invaluable',\n",
              " 'insights',\n",
              " 'support',\n",
              " '............................',\n",
              " '3',\n",
              " 'We',\n",
              " 'wish',\n",
              " 'especially',\n",
              " 'thank',\n",
              " 'MCGM',\n",
              " 'officers',\n",
              " 'Mr.',\n",
              " 'Jagdish',\n",
              " 'Talreja',\n",
              " 'Mr.',\n",
              " 'Dinesh',\n",
              " 'Naik',\n",
              " 'Mr.',\n",
              " 'Hiren',\n",
              " 'Daftardar',\n",
              " 'Ms.',\n",
              " 'Anita',\n",
              " 'Naik',\n",
              " 'continual',\n",
              " 'support',\n",
              " 'since',\n",
              " 'beginning',\n",
              " 'project',\n",
              " 'help',\n",
              " 'towards',\n",
              " 'familiarization',\n",
              " 'data',\n",
              " 'collection',\n",
              " 'They',\n",
              " 'instrumental',\n",
              " 'helping',\n",
              " 'contact',\n",
              " 'various',\n",
              " 'MCGM',\n",
              " 'departments',\n",
              " 'well',\n",
              " 'helping',\n",
              " 'establish',\n",
              " 'contact',\n",
              " 'personnel',\n",
              " 'government',\n",
              " 'departments',\n",
              " 'organizations',\n",
              " 'Many',\n",
              " 'thanks',\n",
              " 'MCGM',\n",
              " 'team',\n",
              " 'deploying',\n",
              " 'personnel',\n",
              " 'particularly',\n",
              " 'Mr.',\n",
              " 'Prasad',\n",
              " 'Gharat',\n",
              " 'extensive',\n",
              " 'field',\n",
              " 'visits',\n",
              " 'helped',\n",
              " 'understanding',\n",
              " 'actual',\n",
              " 'ground',\n",
              " 'conditions',\n",
              " '........................................................................................',\n",
              " '3',\n",
              " 'BEST',\n",
              " '...............................................................................................................................',\n",
              " '.................',\n",
              " '5',\n",
              " 'Brihanmumbai',\n",
              " 'Electric',\n",
              " 'Supply',\n",
              " 'Transport',\n",
              " 'Undertaking',\n",
              " '..............................................................',\n",
              " '5',\n",
              " 'CIDCO',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'City',\n",
              " 'Industrial',\n",
              " 'Development',\n",
              " 'Corporation',\n",
              " '...............................................................................',\n",
              " '5',\n",
              " 'CTP',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Comprehensive',\n",
              " 'Transportation',\n",
              " 'Plan',\n",
              " '...............................................................................................',\n",
              " '5',\n",
              " 'DP',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '..........................................................................................................................',\n",
              " '5',\n",
              " 'DPGM34',\n",
              " '...............................................................................................................................',\n",
              " '..........',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2034',\n",
              " '.......................................................................................',\n",
              " '5',\n",
              " 'DCR',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Control',\n",
              " 'Regulations',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DGPS',\n",
              " '...........................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Digital',\n",
              " 'Global',\n",
              " 'Positioning',\n",
              " 'System',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DPGM',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '...........................................................................................',\n",
              " '5',\n",
              " 'ELU',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Existing',\n",
              " 'Land',\n",
              " 'use',\n",
              " '.............................................................................................................................',\n",
              " '5',\n",
              " 'FSI',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Floor',\n",
              " 'Space',\n",
              " 'Index',\n",
              " '............................................................................................................................',\n",
              " '5',\n",
              " 'GIS',\n",
              " '...............................................................................................................................',\n",
              " '...................',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_list = list()\n",
        "\n",
        "check =  ['Mr.', 'Mrs.', 'Ms.']\n",
        "\n",
        "for idx, token in enumerate(tokens):\n",
        "\n",
        "    if token.startswith(tuple(check)) and idx < (len(tokens)-1):\n",
        "\n",
        "        name = token + tokens[idx+1] + ' ' +  tokens[idx+2]\n",
        "\n",
        "        name_list.append(name)\n",
        "print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh2YOxX1aQv7",
        "outputId": "b079b492-b6d8-4679-e568-ebc01ba454f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile.close()"
      ],
      "metadata": {
        "id": "sBiM2yNQaZ4Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97vFw4u7aqI6",
        "outputId": "418d15c3-6f08-494d-d563-cd1b73523b11"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/244.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx"
      ],
      "metadata": {
        "id": "1zw-VKOAat9b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=open(\"/content/research paper (1).docx\",\"rb\")\n",
        "document=docx.Document(doc)"
      ],
      "metadata": {
        "id": "kiW7ZU_iaw7I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docu=\"\"\n",
        "for para in document.paragraphs:\n",
        "  docu+=para.text\n",
        "print(docu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbqn5n12bJWS",
        "outputId": "4f8dd0e3-2bea-4057-d044-ef92853ff549"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operating A Computer Using Hand GestureB.Mithun Ram, B.Sanjay, B.Yashwanth ,B.Ananth and B.Akshaj Abstract—This research explores developing and implementing a system for operating electronic devices using hand gestures, leveraging deep learning capabilities. Traditional methods of device interaction, such as touch and voice control, have limitations that can be addressed by intuitive and contactless gesture recognition. Our approach involves capturing hand movements using a standard camera, followed by processing these movements through a convolutional neural network (CNN) to classify gestures in real time..I. INTRODUCTIONWith the increasing demand for more natural and intuitive human-computer interactions, traditional methods such as keyboard, mouse, and touch-based controls have significant limitations, particularly for individuals with mobility impairments or in hands-free environments. While voice recognition technologies have made strides, they are not always effective in noisy environments or for users with speech impairments. [1], [2], [3]. Interest in the study of human-robot interactions (HRI) and human-vehicle interactions (HVI) is growing, with a focus on creating more user-friendly, natural, and intuitive engagements [4] There is a need for a more versatile, contactless solution that allows users to control electronic devices and computers using hand gestures. The challenge lies in accurately capturing and interpreting hand gestures in real time using affordable, widely accessible hardware such as standard cameras, while ensuring the system is robust against varying lighting conditions, backgrounds, and diverse hand shapes..[5], To address these challenges, the project proposes the development of a deep learningbased system that uses a standard RGB camera to capture hand gestures and a Convolutional Neural Network (CNN) model to classify these gestures in real time. The system will be trained on a diverse dataset of hand gestures, ensuring high accuracy across different environments and users. To enhance the model’s performance, techniques like data augmentation, transfer learning, and model optimization for real-time processing will be explored. [6]. The system will deliver real-time gesture detection and classification of hand movements, allowing users to perform tasks like controlling a computer interface, navigating applications, and interacting with smart devices. The model's outputs will include the detection of predefined gestures (such as swiping, pinching, or waving), with the potential for custom gestures to be integrated, making the system adaptable to various use cases such as home automation, virtual reality, and assistive technologies. [7]. One of the emerging touch technologies inspired by human skin is textile tactile interfaces [8], which can be deployed over large areas, intricate shapes, and with varying resolutions [9]. However, textile-based technologies encounter challenges such as efficient data and signal processing, deformation, high signal-to-noise ratio (SNR), hysteresis, etc. Despite substantial progress in this field, the integration of these interfaces into consumer electronics remains subpar compared to their conventional counterparts, which predominantly employ lowfootprint rigid materials.In this study, we critically evaluate various hand gesture recognition approaches for the tactile interface based on textiles. We propose a standardized framework for processing tactile signals obtained from the textile interface and a systematic data collection protocol that improves the robustness of data-driven recognition methods. We developed and designed a textile-based tactile sensing system to evaluate the hand-gesture recognition methodologies.II. BACKGROUND & RELATED WORKThe sense of touch in human-machine interaction refers to the ability of a machine or device to detect and respond to tactile stimuli, much like the human sense of touch [10], [11]. This interaction involves the use of tactile sensors or interfaces that enable machines to perceive physical contact, pressure, texture, temperature, and other tactile sensations [12]. In human-machine interaction, incorporating the sense of touch enhances the user experience by providing more natural and intuitive ways of communication and control [13], [14]. It allows users to interact with machines in a manner that mimics real-world tactile interactions, such as pressing buttons, gesturing, or manipulating objects. By integrating tactile feedback into human-machine interfaces, devices can provide users with sensory information that complements other forms of feedback, such as visual or auditory cues. This can improve usability, safety, and efficiency in various applications, including robotics, virtual reality, gaming, healthcare, and automotive systems. The sense of touch is essential for understanding and interacting with objects and other humans, allowing one to distinguish their physical characteristics. It plays a vital role in selfawareness, being able to differentiate the “me” from “not me.” The absence of the sense of touch would widen the gap between what is sensed (the ”raw” data received by the human senses) and what is perceived (the brain‘s interpretation of what the ”raw” data is) [15], [16].Significant advancements have been made in the realm of intelligent systems to enhance human-robot interaction (HRI) and humanmachine interaction (HMI), recognizing their pivotal role in social communication, especially non-verbal exchanges. A variety of devices, such as touchscreens, tactile interfaces, wearable devices, and sensors, have been leveraged to facilitate these interactions [17], [18], [19], [20]. Such efforts underscore the importance of touch in conveying social cues and mood among humans [21]. A range of hardware technologies have been investigated to develop artificial touch systems. In recent decades, researchers have devoted considerable effort to developing these systems using diverse sensor technologies, including resistive, piezoresistive, capacitive, optical, piezoelectric, and acoustic sensors. These sensors can operate individually or in combination to capture various aspects of touch [15]. Despite these strides, replicating the intricate touch-sensing capabilities of humans remains a formidable challenge.Capacitive sensing has established itself as a dominant technology in this field, having gained widespread acclaim since its integration into the Apple iPhone in 2007 [22]. Consequently, mutualcapacitive touchscreens have become the predominant touchsensing technology for mobile devices [23].Investigations are being conducted into innovative methods for users to engage in varying manners and on diverse surfaces. For example, [24] evaluates the algorithmic effectiveness of touch interactions on robots, specifically utilizing the KAIST Motion Expressive Robot (KaMERo). Additionally, [25] employs a flexible and stretchable artificial skin based on the principles of electrical impedance tomography for robotic applications. Meanwhile, [26] utilizes a haptic interface constructed from gridded pressuresensitive conductive ink sheets for a pet-robot interaction.Within touch gesture recognition, a multitude of methods have been explored, revolving around feature extraction or featureengineering. Nearest-Neighbor (NN) classification has effectively facilitated gesture recognition, as indicated by multiple studies, [27], [28], [29], [30]. Other prominent methods, such as statistical classifiers, are also prevalent. For example, [31] uses the Rubine Algorithm (a simple feature extractor and linear classifier [32] designed specifically for Gesture Recognition) to assemble, categorize and selectively choose the best features from a comprehensive feature library for a specified problem. Some studies employ Hidden Markov Models (HMMs) for pattern recognition, such as [33] or Parametric HMMs such as [34]. However, techniques leveraging Hidden Markov Models possess several drawbacks, including the need for conditional independence of observations and the challenge of selecting appropriate hidden states within a generative model. With machine learning techniques, as shown in [35], support vector machine (SVM), multi-layer perceptron (MLP), and discrete wavelet transform (DWT) have shown reasonable performances in classical touch interfaces such as touch screens.The recent focus of researchers has been on deep learning-based techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The authors in [36] presented a CNN-based approach to social gestures[37]. Similarly, researchers in [38] examine the time series properties of RNNs to exploit the temporal characteristic of touch signals for tactile classification, with a specific emphasis on the analysis of long-term dependencies within the data. The reported results suggest that such recurrent networks have significant potential to be used for touch-based gesture recognition.To the best of our knowledge, a systematic study on gesture recognition methods has not been performed on emerging textilebased touch interfaces, which is addressed in this work.III. METHODOLOGIESIn this study, we undertake an evaluation of different hand gesture recognition approaches for a large area tactile sensing interface constructed from conductive textiles. We explore both traditional feature engineering methods and contemporary deep learning techniques to address the challenge of gesture recognition. Leveraging insights from a comprehensive state-of-the-art review, we identify and analyze five prominent algorithms. Furthermore, we devised a standardized experimental protocol to assess the robustness of these methods. Here, ‘robustness’ refers to the capability of the methods to maintain performance across diverse conditions, including variations in hand sizes, movement speeds, applied pressures, and interaction locations. This section provides a detailed exposition of our comparative analysis conducted on the developed textile-based touch interface.A. Hardware: Textile-based Tactile InterfaceThe tactile interface utilized for this study is the TexYZ textilebased capacitive sensor [39]. Uniquely crafted through an embroidery process, this cutting-edge textile tactile surface leverages mutual-capacitance sensing to pinpoint finger interactions on the hand rest, delivering a spatial resolution of 9×9 taxels. The authors in [39], opted to employ mutual-capacitive sensing as opposed toFig. 1: Example of the gesture ”double tap” being performed on the touch interface [39]. The bottom graph represents the evolution through time of the pressure values of each taxel (each one represented with a different color). Each matrix of the top graphs represents a snapshot of the pressure values on the sensor the more rudimentary method of self-capacitance providing: threedegree-of-freedom detection (2D location and pressure xyz), multitouch detection and being less sensitive to EM noise. Furthermore, due to the natural flexibility provided by the textile sensor, it can be seamlessly integrated with any surface, endowing it with tactile properties and expanding its interactive capabilities. [39] utilizes a Cypress Programmable System-on-Chip (PSoC) [40]. The PSoC board is a development board that hosts a PSoC microcontroller unit (MCU) and integrates configurable analog and digital peripheral functions, memory, and a microprocessor on a single chip. The UDP transmission approach was used to transmit data between the sensor and the software, over a local network with the OSC protocol [41] at a rate of 15Hz.B. Hand Gesture ProtocolsTo effectively evaluate the various methods of hand gesture recognition, it is essential to clearly define a set of natural gestures that users would typically perform to interact with the touch interface. To achieve this, we established a repertoire of ten distinct gestures, each varying in complexity. Subsequently, data collection sessions were conducted with the participation of 34 individuals. To test the robustness of the recognition approaches, data collection was carried out under three different inclinations of the tactile interface: parallel to the ground, tilted 30 degrees towards the participant, and tilted 60 degrees towards the participant. In addition, participants were instructed to perform each gesture at three different speeds: regular, fast, and slow. The duration of the recording of each gesture was adjusted to match the length of the gesture, ensuring that the perceived speed of the gesture was consistent and natural across participants. This led to gesture data having variable length.The gesture data collected from each participant was filtered (running average) and normalized. During these sessions, participants were instructed to perform the following touch gestures as part of the data collection process.One Tap: a single tap on the surface.Double Tap: two quick taps on the surface.Swipe Down: dragging the finger toward the lower part of the sensor.Swipe Up: dragging the finger toward the upper part of the sensor.Swipe Right: dragging the finger toward the right side of the sensor.Swipe Left: dragging the finger toward the left side of the sensor.Circle Clock-wise: making a circular motion in a clockwise direction.Circle Counter Clock-wise: making a circular motion in a counter-clockwise direction.Swipe Up Two Fingers: dragging two fingers towards the upper part of the sensor.Swipe Down Two Fingers: dragging two fingers towards the lower part of the sensor.These ten gestures are depicted in Figure 2, with their associated raw signals depicted in Figure 3.(a) Single tap\t(b) Double\t(c) Swipe\t(d) Swipe up\t(e) Swipe tap\tdown with\twith one\tright with one finger\tfinger\tone finger(f) Swipe left (g) Circle (h) Circle (i) Swipe up (j) Swipe with one clockwise counter- with two down with finger clockwise fingers two fingersFig. 2: The 10 Hand Gesture movements associated with therespective classWe obtained two forms of temporal (time-series) gesture data: raw pressure data and trajectory (position of the finger over time) data. The pressure data consists of pressure values from each taxel in a given time frame, resulting in pressure values of N ×N ×T (where N is the dimension of the tactile sensor interface, 9 in this specific case, and T is the duration of the gesture). The trajectory data consisted of the x and y coordinates of up to three detected fingers. These coordinates are defined relative to an origin in the bottom left corner of the sensor array, with x increasing to the right and y increasing upward. The trajectory data are computed through processing performed on the PSoC. In total, we collected 3060 time-series gesture data.C. Data AugmentationThe methods selected for evaluation (feature-engineered as well as deep-learning), require a reasonable amount of gesture data for satisfactory performance. However, collecting large-scale user gesture data is time-consuming and resource-intensive. To address the(a) Single tap\t(b) Double\t(c) Swipe\t(d) Swipe up\t(e) Swipe tap\tdown with\twith one\tright with one finger\tfinger\tone finger(f) Swipe left (g) Circle (h) Circle (i) Swipe up (j) Swipe with one clockwise counter- with two down with finger clockwise fingers two fingersFig. 3: Hand Gestures Raw Signal from the TexYZ, summed over the whole time series lengthlimited amount of available gesture data, the following algorithm has been developed to augment the gesture data. It was imperative to execute data augmentation carefully to ensure that the gesture remained intact without any truncation. The primary objective was to achieve the shift-invariance (both vertically and horizontally) for the gestures. This would enable the model to recognize a gesture irrespective of its location on the tactile sensor. We also evaluated the gesture-recognition methods with and without the proposed augmentation of the collected gesture data.D. Hand Gesture Recognition MethodsIn this section, we present a brief overview of the various hand gesture recognition approaches utilized for comparison in this study.Spatio-temporal Features Approach: In this method, we extract spatial and temporal features from the raw pressure data obtained from each gesture, drawing inspiration from the approach described in [42]. Leveraging the time-dependent nature of the data, we employ Discrete Wavelet Transforms (DWT) for feature extraction. DWT is applied to the raw signal obtained from each taxel across the entire time series, yielding a set of coefficients that capture various signal characteristics.For each wavelet coefficient, we compute statistical features such as the L1 norm, L2 norm, skewness, kurtosis, and standard deviation. We also introduced three spatial features: Mean, Max, and Variance of all the taxels, to obtain correlation information among the taxels during each gesture. An illustrative overview of the approach is presented in Fig.4(A). The computed features are then utilised within different classification algorithms, which are presented in the following section.Touch Pattern Recognition Approach: In this approach, we derive descriptive features (illustrated in Fig.4(B)) from three key parameters of touch gestures: pressure intensity, approximate contact surface area, and duration of gestures. Drawing insights from previous research [18], [43], [37], the features include mainly mean pressure, which aggregates pressure readings across all taxels over time, and maximum pressure, which captures the highest recorded value across all taxels during the gesture. In addition, pressure variability is computed as the average difference in absolute values between sequential frames across all channels, taxels to capture fluctuations in the pressure throughout the gesture.In addition to these temporal metrics, spatial features of the pressure distribution are also computed as row-wise mean pressure, which is calculated to depict the pressure distribution along the length of the sensor array, while column-wise mean pressure represents the distribution along its width. Furthermore, the feature that captures the contact area is also included, which is the maximum and average contact area per frame during the gesture. This helps to distinguish between different types of touch, such as two-finger vs. single-finger gestures.The feature extraction approach described above III-D.1 and III-D.2 was used for classification with different classifiers: K-Nearest Neighbors (KNN), Random Forest (RF), and Support Vector Machine (SVM). The hyperparameters of each were optimized in the train/validation set using cross-validation leave-one-subject-out (5 times). The results of gesture recognition were evaluated and the best performing hyperparameters are presented in Table IV.Deep Learning (DL) Approach- CNN: Deep learning (DL) has surged in popularity, often outperforming traditional methods that rely on feature engineering. Among DL methods, Convolutional Neural Networks (CNNs) have emerged as particularly powerful, especially for array-like data such as images. However, CNNs are inherently designed for static images, which poses a challenge when dealing with dynamic time-series data, such as gestures. To overcome this limitation, we utilize Motion History Images (MHIs) derived from the raw data. An MHI is a grayscale image in which recently activated taxels appear brighter, effectively capturing the motion dynamics of gestures [44]. This approach enables the network to effectively learn and classify dynamic gesture data by leveraging the temporal information encoded in the MHIs.Our selected CNN architecture, as depicted in Fig.4(C), comprises four convolutional layers with Leaky ReLU activation functions, followed by two fully connected layers. The final layer comprises a 10-logit output representing the set of gesture labels.DL Approach- Recurrent Neural Network: RNNs are a widely recognized deep learning architecture that is well suited to handle time series and sequential data, with applications that include gesture recognition, as demonstrated in [38], [45]. However, classical RNNs often encounter challenges such as vanishing gradients, limiting their ability to capture long-range dependencies effectively. To address this, LSTM (Long Short-Term Memory) networks have been introduced, designed to mitigate the limitations of traditional RNNs. In this study, we employ a standard LSTM implementation [45]. Each 9×9 gesture data matrix in each time frame was flattened into a vector of dimensions 81, treating each taxel as an individual feature input to the LSTM network. To avoid overfitting, we set the number of hidden units in the network to 32. Additionally, given that gestures in our dataset vary in length based on their duration, it was essential to standardize the sequence length. We achieve this by padding the gesture time series to ensure a uniform sequence length. A dense fully connected layer with 10-logit output was added after the LSTM layer to facilitate gesture classification. This sequential processing allows the network to recognize the dynamics of gestures, such as speed and direction of motion.DL Approach- Convolutional Neural Network + Long Short Term Memory: We observed that the CNN model struggles to discriminate between gestures of ‘one tap’ and ‘double tap’ because the MHI images for those two gestures are extremely similar. Furthermore, the LSTM model with flat input does not take advantage of the spatial characteristics of gestures. Therefore, to combine the strength of both of these architectures, we evaluated the integration of Convolutional Neural Networks (CNNs) with Long Short-Term Memory (LSTM) [45]. The CNN-LSTM architecture is particularly suitable for the gesture recognition problem, where the input data is both spatially and temporally rich, making it ideal for interpreting tactile sensor data with spatio-temporal dynamics.In our CNN-LSTM network (illustrated in Fig.4(D)), the initial layers are convolutional to extract spatial features, including patterns that are indicative of the type of gesture performed. In contrast to the previous CNN approach, the input is not Motion History Images (MHIs) but the 9×9 arrays produced from every time frame. The CNN is composed of two convolutional layers, with ReLu as an activation function and a Max-Pooling layer. After the convolutional layers, the output feature maps are flattened and fed into the LSTM layers. The LSTM layers are designed with 32 hidden units to handle sequences considering the temporal evolution of gestures. This enables the network to learn not only from the static patterns within each frame but also from the transition of these patterns over time. This combination of spatial feature extraction via CNN and temporal sequence processing via LSTM provides the CNN-LSTM model to effectively classify/recognize complex hand gestures and outperforms other DL approaches, as presented in Sec IV-A. The output of the LSTM is then fed to a fully connected layer of size 10 corresponding to the gestures.IV. RESULTS AND DISCUSSIONThe deployment of this deep learning-based gesture recognition system for device control was designed to ensure smooth, real-time performance and user-friendly interaction. The model was first optimized and tested in a controlled environment before being deployed on edge devices, including smartphones and IoT systems, where device control through gestures would be most practical. Using a compact, lightweight model, such as MobileNet, allowed the system to operate effectively on devices with limited processing power, while edge computing minimized latency by processing data locally rather than relying on remote server. A diverse and representative sample of participants was considered for online testing, including varying hand sizes and demographic characteristics. We present the online recognition performance of the best performing methods (Touch Pattern Recognition with RF classifier, LSTM, and CNN-LSTM) in Table. III. Although the spatio-temporal features method managed to achieve a 98% accuracy rate in offline tests, its efficiency drastically decreased during the initial online tests, making it unsuitable for online evaluation. Moreover, this method required all the time series data in the dataset to have identical length, contrasting other approaches that were invariant to gesture length/duration. In our online tests, we observed that the accuracy for the touch pattern recognition dropped to 71% and 89% for the LSTM model and 93% for the CNN-LSTM model. One limitation of deep learning-based approaches is that the dataset is inherently tied to the specific hardware on which it was collected. Consequently, the introduction of new hardware requires the recollection of data and the retraining of the neural network or the application of domain adaptation techniques. Furthermore, we observed that subsequent uses of the deep learning method resulted in deteriorating performance which can be attributed to the data distribution shift.V. CONCLUSIONIn conclusion, the project successfully demonstrated the potential of using deep learning for gesture recognition to operate devices intuitively and efficiently. By leveraging advanced neural network architectures, including CNNs and LSTMs, the system achieved high accuracy in recognizing a variety of hand gestures, providing a reliable and responsive user experience. The implementation of a user-friendly web GUI facilitated easy interaction, allowing users to engage with the technology seamlessly while offering customization options to enhance usability. VI. ACKNOWLEDGMENTWe extend our heartfelt gratitude to our project guide, Prof. Manasa, for her exceptional guidance, insightful suggestions, and constant encouragement throughout the duration of this project. His expertise and knowledge have been instrumental in shaping our research and ensuring the quality of our work. We are also profoundly grateful to the faculty and staff of the Department of Artificial Intelligence and Machine Learning at Malla Reddy University for providing us with the necessary resources and a conducive environment to pursue our research. Our sincere thanks go to our colleagues and classmates in Section Alpha, Batch Number BT-5, for their collaborative spirit, constructive feedback, and moral support. The stimulating discussions and shared experiences have greatly enriched our learning process and helped us overcome various challenges. We would also like to acknowledge the use of Roboflow for data collection and exploration, as well as Google Colab for providing the computational power and libraries necessary for training our machine learning models. On a personal note, we are grateful to our families for their unwavering support and understanding throughout this demanding journey. Their encouragement and belief in our abilities have been a constant source of motivation. Finally, we extend our gratitude to all those who, directly or indirectly, have contributed to the successful completion of this project. Your support and assistance have been vital to our achievements, and we are deeply appreciative of your contributions. Thank you. .REFERENCESM. Kaboli, D. Feng, K. Yao, P. Lanillos, and G. Cheng, “A tactilebased framework for active object learning and discrimination using multimodal robotic skin,” IEEE Robotics and Automation Letters, vol. 2, no. 4, pp. 2143–2150, 2017.M. Kaboli and G. Cheng, “Robust tactile descriptors for discriminating objects from textural properties via artificial robotic skin,” IEEE Transactions on Robotics, vol. 34, no. 4, pp. 985–1003, 2018.M.K. Bhuyan, et al., Hand pose recognition using geometric features, in: 2011 National Conference on Communications.L. Bretzner, I. Laptev, T. Lindeberg, Hand gesture recognition using multi-scale colour features, hierarchical models... J. Barrho, et al., Finger localization and classification in images based on generalized hough transform\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document.paragraphs)):\n",
        "  print(\"The content of the paragraph \"+str(i)+\" is : \"+document.paragraphs[i].text+\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijd6ATf2bXRo",
        "outputId": "5ecd563b-9d27-4c3a-9d33-c0b042ae4764"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of the paragraph 0 is : Operating A Computer Using Hand Gesture\n",
            "\n",
            "The content of the paragraph 1 is : B.Mithun Ram, B.Sanjay, B.Yashwanth ,B.Ananth and B.Akshaj\n",
            "\n",
            "The content of the paragraph 2 is : \n",
            "\n",
            "The content of the paragraph 3 is :  \n",
            "\n",
            "The content of the paragraph 4 is : Abstract—This research explores developing and implementing a system for operating electronic devices using hand gestures, leveraging deep learning capabilities. Traditional methods of device interaction, such as touch and voice control, have limitations that can be addressed by intuitive and contactless gesture recognition. Our approach involves capturing hand movements using a standard camera, followed by processing these movements through a convolutional neural network (CNN) to classify gestures in real time..\n",
            "\n",
            "The content of the paragraph 5 is : I. INTRODUCTION\n",
            "\n",
            "The content of the paragraph 6 is : With the increasing demand for more natural and intuitive human-computer interactions, traditional methods such as keyboard, mouse, and touch-based controls have significant limitations, particularly for individuals with mobility impairments or in hands-free environments. While voice recognition technologies have made strides, they are not always effective in noisy environments or for users with speech impairments. [1], [2], [3]. Interest in the study of human-robot interactions (HRI) and human-vehicle interactions (HVI) is growing, with a focus on creating more user-friendly, natural, and intuitive engagements [4] There is a need for a more versatile, contactless solution that allows users to control electronic devices and computers using hand gestures. The challenge lies in accurately capturing and interpreting hand gestures in real time using affordable, widely accessible hardware such as standard cameras, while ensuring the system is robust against varying lighting conditions, backgrounds, and diverse hand shapes..\n",
            "\n",
            "The content of the paragraph 7 is : [5], To address these challenges, the project proposes the development of a deep learningbased system that uses a standard RGB camera to capture hand gestures and a Convolutional Neural Network (CNN) model to classify these gestures in real time. The system will be trained on a diverse dataset of hand gestures, ensuring high accuracy across different environments and users. To enhance the model’s performance, techniques like data augmentation, transfer learning, and model optimization for real-time processing will be explored. [6]. The system will deliver real-time gesture detection and classification of hand movements, allowing users to perform tasks like controlling a computer interface, navigating applications, and interacting with smart devices. The model's outputs will include the detection of predefined gestures (such as swiping, pinching, or waving), with the potential for custom gestures to be integrated, making the system adaptable to various use cases such as home automation, virtual reality, and assistive technologies. [7]. One of the emerging touch technologies inspired by human skin is textile tactile interfaces [8], which can be deployed over large areas, intricate shapes, and with varying resolutions [9]. However, textile-based technologies encounter challenges such as efficient data and signal processing, deformation, high signal-to-noise ratio (SNR), hysteresis, etc. Despite substantial progress in this field, the integration of these interfaces into consumer electronics remains subpar compared to their conventional counterparts, which predominantly employ lowfootprint rigid materials.\n",
            "\n",
            "The content of the paragraph 8 is : In this study, we critically evaluate various hand gesture recognition approaches for the tactile interface based on textiles. We propose a standardized framework for processing tactile signals obtained from the textile interface and a systematic data collection protocol that improves the robustness of data-driven recognition methods. We developed and designed a textile-based tactile sensing system to evaluate the hand-gesture recognition methodologies.\n",
            "\n",
            "The content of the paragraph 9 is : II. BACKGROUND & RELATED WORK\n",
            "\n",
            "The content of the paragraph 10 is : The sense of touch in human-machine interaction refers to the ability of a machine or device to detect and respond to tactile stimuli, much like the human sense of touch [10], [11]. This interaction involves the use of tactile sensors or interfaces that enable machines to perceive physical contact, pressure, texture, temperature, and other tactile sensations [12]. In human-machine interaction, incorporating the sense of touch enhances the user experience by providing more natural and intuitive ways of communication and control [13], [14]. It allows users to interact with machines in a manner that mimics real-world tactile interactions, such as pressing buttons, gesturing, or manipulating objects. By integrating tactile feedback into human-machine interfaces, devices can provide users with sensory information that complements other forms of feedback, such as visual or auditory cues. This can improve usability, safety, and efficiency in various applications, including robotics, virtual reality, gaming, healthcare, and automotive systems. The sense of touch is essential for understanding and interacting with objects and other humans, allowing one to distinguish their physical characteristics. It plays a vital role in selfawareness, being able to differentiate the “me” from “not me.” The absence of the sense of touch would widen the gap between what is sensed (the ”raw” data received by the human senses) and what is perceived (the brain‘s interpretation of what the ”raw” data is) [15], [16].\n",
            "\n",
            "The content of the paragraph 11 is : Significant advancements have been made in the realm of intelligent systems to enhance human-robot interaction (HRI) and humanmachine interaction (HMI), recognizing their pivotal role in social communication, especially non-verbal exchanges. A variety of devices, such as touchscreens, tactile interfaces, wearable devices, and sensors, have been leveraged to facilitate these interactions [17], [18], [19], [20]. Such efforts underscore the importance of touch in conveying social cues and mood among humans [21]. A range of hardware technologies have been investigated to develop artificial touch systems. In recent decades, researchers have devoted considerable effort to developing these systems using diverse sensor technologies, including resistive, piezoresistive, capacitive, optical, piezoelectric, and acoustic sensors. These sensors can operate individually or in combination to capture various aspects of touch [15]. Despite these strides, replicating the intricate touch-sensing capabilities of humans remains a formidable challenge.\n",
            "\n",
            "The content of the paragraph 12 is : Capacitive sensing has established itself as a dominant technology in this field, having gained widespread acclaim since its integration into the Apple iPhone in 2007 [22]. Consequently, mutualcapacitive touchscreens have become the predominant touchsensing technology for mobile devices [23].\n",
            "\n",
            "The content of the paragraph 13 is : Investigations are being conducted into innovative methods for users to engage in varying manners and on diverse surfaces. For example, [24] evaluates the algorithmic effectiveness of touch interactions on robots, specifically utilizing the KAIST Motion Expressive Robot (KaMERo). Additionally, [25] employs a flexible and stretchable artificial skin based on the principles of electrical impedance tomography for robotic applications. Meanwhile, [26] utilizes a haptic interface constructed from gridded pressuresensitive conductive ink sheets for a pet-robot interaction.\n",
            "\n",
            "The content of the paragraph 14 is : Within touch gesture recognition, a multitude of methods have been explored, revolving around feature extraction or featureengineering. Nearest-Neighbor (NN) classification has effectively facilitated gesture recognition, as indicated by multiple studies, [27], [28], [29], [30]. Other prominent methods, such as statistical classifiers, are also prevalent. For example, [31] uses the Rubine Algorithm (a simple feature extractor and linear classifier [32] designed specifically for Gesture Recognition) to assemble, categorize and selectively choose the best features from a comprehensive feature library for a specified problem. Some studies employ Hidden Markov Models (HMMs) for pattern recognition, such as [33] or Parametric HMMs such as [34]. However, techniques leveraging Hidden Markov Models possess several drawbacks, including the need for conditional independence of observations and the challenge of selecting appropriate hidden states within a generative model. With machine learning techniques, as shown in [35], support vector machine (SVM), multi-layer perceptron (MLP), and discrete wavelet transform (DWT) have shown reasonable performances in classical touch interfaces such as touch screens.\n",
            "\n",
            "The content of the paragraph 15 is : The recent focus of researchers has been on deep learning-based techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The authors in [36] presented a CNN-based approach to social gestures[37]. Similarly, researchers in [38] examine the time series properties of RNNs to exploit the temporal characteristic of touch signals for tactile classification, with a specific emphasis on the analysis of long-term dependencies within the data. The reported results suggest that such recurrent networks have significant potential to be used for touch-based gesture recognition.\n",
            "\n",
            "The content of the paragraph 16 is : To the best of our knowledge, a systematic study on gesture recognition methods has not been performed on emerging textilebased touch interfaces, which is addressed in this work.\n",
            "\n",
            "The content of the paragraph 17 is : III. METHODOLOGIES\n",
            "\n",
            "The content of the paragraph 18 is : In this study, we undertake an evaluation of different hand gesture recognition approaches for a large area tactile sensing interface constructed from conductive textiles. We explore both traditional feature engineering methods and contemporary deep learning techniques to address the challenge of gesture recognition. Leveraging insights from a comprehensive state-of-the-art review, we identify and analyze five prominent algorithms. Furthermore, we devised a standardized experimental protocol to assess the robustness of these methods. Here, ‘robustness’ refers to the capability of the methods to maintain performance across diverse conditions, including variations in hand sizes, movement speeds, applied pressures, and interaction locations. This section provides a detailed exposition of our comparative analysis conducted on the developed textile-based touch interface.\n",
            "\n",
            "The content of the paragraph 19 is : A. Hardware: Textile-based Tactile Interface\n",
            "\n",
            "The content of the paragraph 20 is : The tactile interface utilized for this study is the TexYZ textilebased capacitive sensor [39]. Uniquely crafted through an embroidery process, this cutting-edge textile tactile surface leverages mutual-capacitance sensing to pinpoint finger interactions on the hand rest, delivering a spatial resolution of 9×9 taxels. The authors in [39], opted to employ mutual-capacitive sensing as opposed to\n",
            "\n",
            "The content of the paragraph 21 is : \n",
            "\n",
            "The content of the paragraph 22 is : Fig. 1: Example of the gesture ”double tap” being performed on the touch interface [39]. The bottom graph represents the evolution through time of the pressure values of each taxel (each one represented with a different color). Each matrix of the top graphs represents a snapshot of the pressure values on the sensor the more rudimentary method of self-capacitance providing: threedegree-of-freedom detection (2D location and pressure xyz), multitouch detection and being less sensitive to EM noise. Furthermore, due to the natural flexibility provided by the textile sensor, it can be seamlessly integrated with any surface, endowing it with tactile properties and expanding its interactive capabilities. [39] utilizes a Cypress Programmable System-on-Chip (PSoC) [40]. The PSoC board is a development board that hosts a PSoC microcontroller unit (MCU) and integrates configurable analog and digital peripheral functions, memory, and a microprocessor on a single chip. The UDP transmission approach was used to transmit data between the sensor and the software, over a local network with the OSC protocol [41] at a rate of 15Hz.\n",
            "\n",
            "The content of the paragraph 23 is : B. Hand Gesture Protocols\n",
            "\n",
            "The content of the paragraph 24 is : To effectively evaluate the various methods of hand gesture recognition, it is essential to clearly define a set of natural gestures that users would typically perform to interact with the touch interface. To achieve this, we established a repertoire of ten distinct gestures, each varying in complexity. Subsequently, data collection sessions were conducted with the participation of 34 individuals. To test the robustness of the recognition approaches, data collection was carried out under three different inclinations of the tactile interface: parallel to the ground, tilted 30 degrees towards the participant, and tilted 60 degrees towards the participant. In addition, participants were instructed to perform each gesture at three different speeds: regular, fast, and slow. The duration of the recording of each gesture was adjusted to match the length of the gesture, ensuring that the perceived speed of the gesture was consistent and natural across participants. This led to gesture data having variable length.\n",
            "\n",
            "The content of the paragraph 25 is : The gesture data collected from each participant was filtered (running average) and normalized. During these sessions, participants were instructed to perform the following touch gestures as part of the data collection process.\n",
            "\n",
            "The content of the paragraph 26 is : One Tap: a single tap on the surface.\n",
            "\n",
            "The content of the paragraph 27 is : Double Tap: two quick taps on the surface.\n",
            "\n",
            "The content of the paragraph 28 is : Swipe Down: dragging the finger toward the lower part of the sensor.\n",
            "\n",
            "The content of the paragraph 29 is : Swipe Up: dragging the finger toward the upper part of the sensor.\n",
            "\n",
            "The content of the paragraph 30 is : Swipe Right: dragging the finger toward the right side of the sensor.\n",
            "\n",
            "The content of the paragraph 31 is : Swipe Left: dragging the finger toward the left side of the sensor.\n",
            "\n",
            "The content of the paragraph 32 is : Circle Clock-wise: making a circular motion in a clockwise direction.\n",
            "\n",
            "The content of the paragraph 33 is : Circle Counter Clock-wise: making a circular motion in a counter-clockwise direction.\n",
            "\n",
            "The content of the paragraph 34 is : Swipe Up Two Fingers: dragging two fingers towards the upper part of the sensor.\n",
            "\n",
            "The content of the paragraph 35 is : Swipe Down Two Fingers: dragging two fingers towards the lower part of the sensor.\n",
            "\n",
            "The content of the paragraph 36 is : These ten gestures are depicted in Figure 2, with their associated raw signals depicted in Figure 3.\n",
            "\n",
            "The content of the paragraph 37 is : \n",
            "\n",
            "The content of the paragraph 38 is : (a) Single tap\t(b) Double\t(c) Swipe\t(d) Swipe up\t(e) Swipe tap\tdown with\twith one\tright with one finger\tfinger\tone finger\n",
            "\n",
            "The content of the paragraph 39 is : \n",
            "\n",
            "The content of the paragraph 40 is : (f) Swipe left (g) Circle (h) Circle (i) Swipe up (j) Swipe with one clockwise counter- with two down with finger clockwise fingers two fingers\n",
            "\n",
            "The content of the paragraph 41 is : Fig. 2: The 10 Hand Gesture movements associated with the\n",
            "\n",
            "The content of the paragraph 42 is : respective class\n",
            "\n",
            "The content of the paragraph 43 is : We obtained two forms of temporal (time-series) gesture data: raw pressure data and trajectory (position of the finger over time) data. The pressure data consists of pressure values from each taxel in a given time frame, resulting in pressure values of N ×N ×T (where N is the dimension of the tactile sensor interface, 9 in this specific case, and T is the duration of the gesture). The trajectory data consisted of the x and y coordinates of up to three detected fingers. These coordinates are defined relative to an origin in the bottom left corner of the sensor array, with x increasing to the right and y increasing upward. The trajectory data are computed through processing performed on the PSoC. In total, we collected 3060 time-series gesture data.\n",
            "\n",
            "The content of the paragraph 44 is : C. Data Augmentation\n",
            "\n",
            "The content of the paragraph 45 is : The methods selected for evaluation (feature-engineered as well as deep-learning), require a reasonable amount of gesture data for satisfactory performance. However, collecting large-scale user gesture data is time-consuming and resource-intensive. To address the\n",
            "\n",
            "The content of the paragraph 46 is : \n",
            "\n",
            "The content of the paragraph 47 is : (a) Single tap\t(b) Double\t(c) Swipe\t(d) Swipe up\t(e) Swipe tap\tdown with\twith one\tright with one finger\tfinger\tone finger\n",
            "\n",
            "The content of the paragraph 48 is : \n",
            "\n",
            "The content of the paragraph 49 is : (f) Swipe left (g) Circle (h) Circle (i) Swipe up (j) Swipe with one clockwise counter- with two down with finger clockwise fingers two fingers\n",
            "\n",
            "The content of the paragraph 50 is : Fig. 3: Hand Gestures Raw Signal from the TexYZ, summed over the whole time series length\n",
            "\n",
            "The content of the paragraph 51 is : limited amount of available gesture data, the following algorithm has been developed to augment the gesture data. It was imperative to execute data augmentation carefully to ensure that the gesture remained intact without any truncation. The primary objective was to achieve the shift-invariance (both vertically and horizontally) for the gestures. This would enable the model to recognize a gesture irrespective of its location on the tactile sensor. We also evaluated the gesture-recognition methods with and without the proposed augmentation of the collected gesture data.\n",
            "\n",
            "The content of the paragraph 52 is : D. Hand Gesture Recognition Methods\n",
            "\n",
            "The content of the paragraph 53 is : In this section, we present a brief overview of the various hand gesture recognition approaches utilized for comparison in this study.\n",
            "\n",
            "The content of the paragraph 54 is : Spatio-temporal Features Approach: In this method, we extract spatial and temporal features from the raw pressure data obtained from each gesture, drawing inspiration from the approach described in [42]. Leveraging the time-dependent nature of the data, we employ Discrete Wavelet Transforms (DWT) for feature extraction. DWT is applied to the raw signal obtained from each taxel across the entire time series, yielding a set of coefficients that capture various signal characteristics.\n",
            "\n",
            "The content of the paragraph 55 is : For each wavelet coefficient, we compute statistical features such as the L1 norm, L2 norm, skewness, kurtosis, and standard deviation. We also introduced three spatial features: Mean, Max, and Variance of all the taxels, to obtain correlation information among the taxels during each gesture. An illustrative overview of the approach is presented in Fig.4(A). The computed features are then utilised within different classification algorithms, which are presented in the following section.\n",
            "\n",
            "The content of the paragraph 56 is : Touch Pattern Recognition Approach: In this approach, we derive descriptive features (illustrated in Fig.4(B)) from three key parameters of touch gestures: pressure intensity, approximate contact surface area, and duration of gestures. Drawing insights from previous research [18], [43], [37], the features include mainly mean pressure, which aggregates pressure readings across all taxels over time, and maximum pressure, which captures the highest recorded value across all taxels during the gesture. In addition, pressure variability is computed as the average difference in absolute values between sequential frames across all channels, taxels to capture fluctuations in the pressure throughout the gesture.\n",
            "\n",
            "The content of the paragraph 57 is : In addition to these temporal metrics, spatial features of the pressure distribution are also computed as row-wise mean pressure, which is calculated to depict the pressure distribution along the length of the sensor array, while column-wise mean pressure represents the distribution along its width. Furthermore, the feature that captures the contact area is also included, which is the maximum and average contact area per frame during the gesture. This helps to distinguish between different types of touch, such as two-finger vs. single-finger gestures.\n",
            "\n",
            "The content of the paragraph 58 is : The feature extraction approach described above III-D.1 and III-\n",
            "\n",
            "The content of the paragraph 59 is : D.2 was used for classification with different classifiers: K-Nearest Neighbors (KNN), Random Forest (RF), and Support Vector Machine (SVM). The hyperparameters of each were optimized in the train/validation set using cross-validation leave-one-subject-out (5 times). The results of gesture recognition were evaluated and the best performing hyperparameters are presented in Table IV.\n",
            "\n",
            "The content of the paragraph 60 is : Deep Learning (DL) Approach- CNN: Deep learning (DL) has surged in popularity, often outperforming traditional methods that rely on feature engineering. Among DL methods, Convolutional Neural Networks (CNNs) have emerged as particularly powerful, especially for array-like data such as images. However, CNNs are inherently designed for static images, which poses a challenge when dealing with dynamic time-series data, such as gestures. To overcome this limitation, we utilize Motion History Images (MHIs) derived from the raw data. An MHI is a grayscale image in which recently activated taxels appear brighter, effectively capturing the motion dynamics of gestures [44]. This approach enables the network to effectively learn and classify dynamic gesture data by leveraging the temporal information encoded in the MHIs.\n",
            "\n",
            "The content of the paragraph 61 is : Our selected CNN architecture, as depicted in Fig.4(C), comprises four convolutional layers with Leaky ReLU activation functions, followed by two fully connected layers. The final layer comprises a 10-logit output representing the set of gesture labels.\n",
            "\n",
            "The content of the paragraph 62 is : DL Approach- Recurrent Neural Network: RNNs are a widely recognized deep learning architecture that is well suited to handle time series and sequential data, with applications that include gesture recognition, as demonstrated in [38], [45]. However, classical RNNs often encounter challenges such as vanishing gradients, limiting their ability to capture long-range dependencies effectively. To address this, LSTM (Long Short-Term Memory) networks have been introduced, designed to mitigate the limitations of traditional RNNs. In this study, we employ a standard LSTM implementation [45]. Each 9×9 gesture data matrix in each time frame was flattened into a vector of dimensions 81, treating each taxel as an individual feature input to the LSTM network. To avoid overfitting, we set the number of hidden units in the network to 32. Additionally, given that gestures in our dataset vary in length based on their duration, it was essential to standardize the sequence length. We achieve this by padding the gesture time series to ensure a uniform sequence length. A dense fully connected layer with 10-logit output was added after the LSTM layer to facilitate gesture classification. This sequential processing allows the network to recognize the dynamics of gestures, such as speed and direction of motion.\n",
            "\n",
            "The content of the paragraph 63 is : DL Approach- Convolutional Neural Network + Long Short Term Memory: We observed that the CNN model struggles to discriminate between gestures of ‘one tap’ and ‘double tap’ because the MHI images for those two gestures are extremely similar. Furthermore, the LSTM model with flat input does not take advantage of the spatial characteristics of gestures. Therefore, to combine the strength of both of these architectures, we evaluated the integration of Convolutional Neural Networks (CNNs) with Long Short-Term Memory (LSTM) [45]. The CNN-LSTM architecture is particularly suitable for the gesture recognition problem, where the input data is both spatially and temporally rich, making it ideal for interpreting tactile sensor data with spatio-temporal dynamics.\n",
            "\n",
            "The content of the paragraph 64 is : In our CNN-LSTM network (illustrated in Fig.4(D)), the initial layers are convolutional to extract spatial features, including patterns that are indicative of the type of gesture performed. In contrast to the previous CNN approach, the input is not Motion History Images (MHIs) but the 9×9 arrays produced from every time frame. The CNN is composed of two convolutional layers, with ReLu as an activation function and a Max-Pooling layer. After the convolutional layers, the output feature maps are flattened and fed into the LSTM layers. The LSTM layers are designed with 32 hidden units to handle sequences considering the temporal evolution of gestures. This enables the network to learn not only from the static patterns within each frame but also from the transition of these patterns over time. This combination of spatial feature extraction via CNN and temporal sequence processing via LSTM provides the CNN-LSTM model to effectively classify/recognize complex hand gestures and outperforms other DL approaches, as presented in Sec IV-A. The output of the LSTM is then fed to a fully connected layer of size 10 corresponding to the gestures.\n",
            "\n",
            "The content of the paragraph 65 is : IV. RESULTS AND DISCUSSION\n",
            "\n",
            "The content of the paragraph 66 is : The deployment of this deep learning-based gesture recognition system for device control was designed to ensure smooth, real-time performance and user-friendly interaction. The model was first optimized and tested in a controlled environment before being deployed on edge devices, including smartphones and IoT systems, where device control through gestures would be most practical. Using a compact, lightweight model, such as MobileNet, allowed the system to operate effectively on devices with limited processing power, while edge computing minimized latency by processing data locally rather than relying on remote server\n",
            "\n",
            "The content of the paragraph 67 is : . A diverse and representative sample of participants was considered for online testing, including varying hand sizes and demographic characteristics. We present the online recognition performance of the best performing methods (Touch Pattern Recognition with RF classifier, LSTM, and CNN-LSTM) in Table. III. Although the spatio-temporal features method managed to achieve a 98% accuracy rate in offline tests, its efficiency drastically decreased during the initial online tests, making it unsuitable for online evaluation. Moreover, this method required all the time series data in the dataset to have identical length, contrasting other approaches that were invariant to gesture length/duration. In our online tests, we observed that the accuracy for the touch pattern recognition dropped to 71% and 89% for the LSTM model and 93% for the CNN-LSTM model. One limitation of deep learning-based approaches is that the dataset is inherently tied to the specific hardware on which it was collected. Consequently, the introduction of new hardware requires the recollection of data and the retraining of the neural network or the application of domain adaptation techniques. Furthermore, we observed that subsequent uses of the deep learning method resulted in deteriorating performance which can be attributed to the data distribution shift.\n",
            "\n",
            "The content of the paragraph 68 is : V. CONCLUSION\n",
            "\n",
            "The content of the paragraph 69 is : In conclusion, the project successfully demonstrated the potential of using deep learning for gesture recognition to operate devices intuitively and efficiently. By leveraging advanced neural network architectures, including CNNs and LSTMs, the system achieved high accuracy in recognizing a variety of hand gestures, providing a reliable and responsive user experience. The implementation of a user-friendly web GUI facilitated easy interaction, allowing users to engage with the technology seamlessly while offering customization options to enhance usability. \n",
            "\n",
            "The content of the paragraph 70 is : \n",
            "\n",
            "The content of the paragraph 71 is : VI. ACKNOWLEDGMENT\n",
            "\n",
            "The content of the paragraph 72 is : We extend our heartfelt gratitude to our project guide, Prof. Manasa, for her exceptional guidance, insightful suggestions, and constant encouragement throughout the duration of this project. His expertise and knowledge have been instrumental in shaping our research and ensuring the quality of our work. We are also profoundly grateful to the faculty and staff of the Department of Artificial Intelligence and Machine Learning at Malla Reddy University for providing us with the necessary resources and a conducive environment to pursue our research. \n",
            "\n",
            "The content of the paragraph 73 is : Our sincere thanks go to our colleagues and classmates in Section Alpha, Batch Number BT-5, for their collaborative spirit, constructive feedback, and moral support. The stimulating discussions and shared experiences have greatly enriched our learning process and helped us overcome various challenges. We would also like to acknowledge the use of Roboflow for data collection and exploration, as well as Google Colab for providing the computational power and libraries necessary for training our machine learning models. \n",
            "\n",
            "The content of the paragraph 74 is : On a personal note, we are grateful to our families for their unwavering support and understanding throughout this demanding journey. Their encouragement and belief in our abilities have been a constant source of motivation. Finally, we extend our gratitude to all those who, directly or indirectly, have contributed to the successful completion of this project. Your support and assistance have been vital to our achievements, and we are deeply appreciative of your contributions. Thank you. \n",
            "\n",
            "The content of the paragraph 75 is : .\n",
            "\n",
            "The content of the paragraph 76 is : REFERENCES\n",
            "\n",
            "The content of the paragraph 77 is : M. Kaboli, D. Feng, K. Yao, P. Lanillos, and G. Cheng, “A tactilebased framework for active object learning and discrimination using multimodal robotic skin,” IEEE Robotics and Automation Letters, vol. 2, no. 4, pp. 2143–2150, 2017.\n",
            "\n",
            "The content of the paragraph 78 is : M. Kaboli and G. Cheng, “Robust tactile descriptors for discriminating objects from textural properties via artificial robotic skin,” IEEE Transactions on Robotics, vol. 34, no. 4, pp. 985–1003, 2018.\n",
            "\n",
            "The content of the paragraph 79 is : M.K. Bhuyan, et al., Hand pose recognition using geometric features, in: 2011 National Conference on Communications.\n",
            "\n",
            "The content of the paragraph 80 is : L. Bretzner, I. Laptev, T. Lindeberg, Hand gesture recognition using multi-scale colour features, hierarchical models... \n",
            "\n",
            "The content of the paragraph 81 is : J. Barrho, et al., Finger localization and classification in images based on generalized hough transform\n",
            "\n",
            "The content of the paragraph 82 is : \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X_OW2CzcyDi",
        "outputId": "b778a0e7-12d0-4435-9975-9ad69d89832f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as urllib2\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "GY83BwwUc8UC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "html_doc=response.read()"
      ],
      "metadata": {
        "id": "vBobe9o3dIi2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup=BeautifulSoup(html_doc,'html.parser')\n",
        "strhtm=soup.prettify()\n",
        "print(strhtm[:5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfvFWrb1dZdn",
        "outputId": "37f1420a-31f7-41d9-dae3-33aebecbcd55"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <title>\n",
            "   Natural language processing - Wikipedia\n",
            "  </title>\n",
            "  <script>\n",
            "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
            "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"8110193a-7e31-45cd-b950-ec2910f9b347\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1274942014,\"wgRevisionId\":1274942014,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"All accuracy disputes\",\"Accuracy disputes from December 2013\",\"Harv and Sfn no-target errors\",\"CS1 errors: periodical ignored\",\"CS1 maint: location\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from May 2024\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from May 2024\",\"Commons category link from Wikidata\",\n",
            "\"Natural language processing\",\"Computational fields of study\",\"Computational linguistics\",\"Speech recognition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Natural_language_processing\",\"wgRelevantArticleId\":21652,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\n",
            "\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q30642\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\n",
            "\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdGRg9iMdwx3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}